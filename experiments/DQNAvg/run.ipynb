{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from florl.common.util import aggregate_weighted_average, stateful_client\n",
    "\n",
    "from dqnavg import *\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_CLIENTS = 10\n",
    "TOTAL_ROUNDS = 10\n",
    "FRAMES_PER_ROUND = 100\n",
    "EXPERIMENT_REPEATS = 30\n",
    "\n",
    "config = DictConfig({\n",
    "    \"rl\": {\n",
    "        \"env\": {\n",
    "            \"name\": \"CartPole-v1\"\n",
    "        },\n",
    "        \"algorithm\": {\n",
    "            \"gamma\": 0.99,\n",
    "            \"tau\": 0.005,\n",
    "            \"lr\": 0.001,\n",
    "            \"update_frequency\": 1,\n",
    "            \"clip_grad_norm\": 1,\n",
    "            \"critic\": {\n",
    "                \"features\": 64\n",
    "            }\n",
    "        },\n",
    "        \"memory\": {\n",
    "            \"type\": \"experience_replay\",\n",
    "            \"capacity\": TOTAL_ROUNDS * FRAMES_PER_ROUND\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"initial_collection_size\": 512,\n",
    "            \"minibatch_size\": 32\n",
    "        }\n",
    "    },\n",
    "    \"fl\": {\n",
    "        \"train_config\": {\n",
    "            \"frames\": FRAMES_PER_ROUND,\n",
    "        },\n",
    "        \"evaluate_config\": {\n",
    "            \"evaluation_repeats\": 1 \n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "train_config = OmegaConf.to_container(config[\"fl\"][\"train_config\"])\n",
    "evaluate_config = OmegaConf.to_container(config[\"fl\"][\"evaluate_config\"])\n",
    "\n",
    "def _on_fit_config_fn(server_round: int):\n",
    "    return train_config | {\"server_round\": server_round}\n",
    "def _on_evaluate_config_fn(server_round: int):\n",
    "    return evaluate_config | {\"server_round\": server_round}\n",
    "\n",
    "client_factory = DQNClientFactory(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = []\n",
    "for seed in tqdm(range(EXPERIMENT_REPEATS)):\n",
    "    client = client_factory.create_dqn_client(seed, config[\"rl\"])\n",
    "\n",
    "    # Manually run through the training loop\n",
    "    hist_fit = []\n",
    "    evaluation_reward = []\n",
    "    for simulated_rounds in range(TOTAL_ROUNDS):\n",
    "        _, metrics = client.train(config[\"fl\"][\"train_config\"])\n",
    "        hist_fit.append(metrics)\n",
    "        evaluation_reward.append(client._evaluator.evaluate(client.policy))\n",
    "\n",
    "    baseline_results.append((hist_fit, evaluation_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_WS = \"florl_ws\"\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    on_fit_config_fn = _on_fit_config_fn,\n",
    "    on_evaluate_config_fn= _on_evaluate_config_fn,\n",
    "    fit_metrics_aggregation_fn=aggregate_weighted_average,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_weighted_average, \n",
    "    accept_failures=False\n",
    ")\n",
    "\n",
    "federated_results = []\n",
    "\n",
    "for seed in tqdm(range(EXPERIMENT_REPEATS)):\n",
    "    shutil.rmtree(CONTEXT_WS)\n",
    "\n",
    "    @stateful_client\n",
    "    def build_client(cid: str) -> fl.client.Client:\n",
    "        cid = int(cid) + seed * NUM_CLIENTS\n",
    "        return client_factory.create_dqn_client(cid, config=config[\"rl\"])\n",
    "\n",
    "    hist = fl.simulation.start_simulation(\n",
    "        client_fn=build_client,\n",
    "        client_resources={'num_cpus': 1},\n",
    "        config=fl.server.ServerConfig(num_rounds=TOTAL_ROUNDS),\n",
    "        num_clients = NUM_CLIENTS,\n",
    "        strategy = strategy\n",
    "    )\n",
    "\n",
    "    federated_results.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2)\n",
    "fig.set_size_inches(12,5)\n",
    "fig.suptitle(\"DQN/DQNAvg on CartPole\")\n",
    "\n",
    "rounds = list(range(TOTAL_ROUNDS))\n",
    "\n",
    "# Loss\n",
    "baseline_losses = np.array([[s['loss'] for s in ex[0]] for ex in baseline_results])\n",
    "baseline_losses_mean = baseline_losses.mean(axis=0)\n",
    "baseline_losses_std = baseline_losses.std(axis=0)\n",
    "federated_losses = np.array([[x[1]['all'] for x in hist.metrics_distributed_fit[\"loss\"]] for hist in federated_results])\n",
    "federated_losses = federated_losses.transpose((0,2,1,3)).reshape((EXPERIMENT_REPEATS*NUM_CLIENTS, TOTAL_ROUNDS, 2))[:,:,1]\n",
    "federated_losses_mean = federated_losses.mean(axis=0)\n",
    "federated_losses_std = federated_losses.std(axis=0)\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(rounds, baseline_losses_mean, color=\"r\", label=\"DQN\")\n",
    "ax.fill_between(\n",
    "    x=rounds,\n",
    "    y1=baseline_losses_mean-baseline_losses_std*1.96,\n",
    "    y2=baseline_losses_mean+baseline_losses_std*1.96,\n",
    "    alpha=0.2,\n",
    "    color=\"r\"\n",
    ")\n",
    "ax.plot(rounds, federated_losses_mean, color=\"g\", label=\"DQNAvg\")\n",
    "ax.fill_between(\n",
    "    x=rounds,\n",
    "    y1=federated_losses_mean-baseline_losses_std*1.96,\n",
    "    y2=federated_losses_mean+baseline_losses_std*1.96,\n",
    "    alpha=0.2,\n",
    "    color=\"g\"\n",
    ")\n",
    "ax.set_title(\"Training Loss\")\n",
    "ax.set_ylabel(\"Average Loss\")\n",
    "ax.set_xlabel(\"Round\")\n",
    "\n",
    "# Evaluation Reward\n",
    "baseline_rewards = np.array([ex[1] for ex in baseline_results])\n",
    "baseline_rewards_mean = baseline_rewards.mean(axis=0)\n",
    "baseline_rewards_std = baseline_rewards.std(axis=0)\n",
    "federated_rewards = np.array([[x[1]['all'] for x in hist.metrics_distributed[\"reward\"]] for hist in federated_results])\n",
    "federated_rewards = federated_rewards.transpose((0,2,1,3)).reshape((EXPERIMENT_REPEATS*NUM_CLIENTS, TOTAL_ROUNDS, 2))[:,:,1]\n",
    "federated_rewards_mean = federated_rewards.mean(axis=0)\n",
    "federated_rewards_std = federated_rewards.std(axis=0)\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(rounds, baseline_rewards_mean, color=\"r\", label=\"DQN\")\n",
    "ax.fill_between(\n",
    "    x=rounds,\n",
    "    y1=baseline_rewards_mean-baseline_rewards_std*1.96,\n",
    "    y2=baseline_rewards_mean+baseline_rewards_std*1.96,\n",
    "    alpha=0.2,\n",
    "    color=\"r\"\n",
    ")\n",
    "ax.plot(rounds, federated_rewards_mean, color=\"g\", label=\"DQNAvg\")\n",
    "ax.fill_between(\n",
    "    x=rounds,\n",
    "    y1=federated_rewards_mean-federated_rewards_std*1.96,\n",
    "    y2=federated_rewards_mean+federated_rewards_std*1.96,\n",
    "    alpha=0.2,\n",
    "    color=\"g\"\n",
    ")\n",
    "ax.set_title(\"Evaluation Reward\")\n",
    "ax.set_ylabel(\"Average Episode Reward\")\n",
    "ax.set_xlabel(\"Round\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
